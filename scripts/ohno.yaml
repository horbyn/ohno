apiVersion: v1
kind: Namespace
metadata:
  name: ohno
---
apiVersion: v1
kind: Secret
metadata:
  name: etcd-certs
  namespace: ohno
type: Opaque
data:
  # export CA_CRT=$(cat /etc/kubernetes/pki/etcd/ca.crt | base64)
  ca.crt: "${CA_CRT}" # 从环境变量传入 /etc/kubernetes/pki/etcd/ca.crt 的 base64 编码
  # export CLIENT_CRT=$(cat /etc/kubernetes/pki/etcd/healthcheck-client.crt | base64)
  client.crt: "${CLIENT_CRT}"
  # export CLIENT_KEY=$(cat /etc/kubernetes/pki/etcd/healthcheck-client.key | base64)
  client.key: "${CLIENT_KEY}"
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ohnod
  namespace: ohno
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: ohnod
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ohnod
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ohnod
subjects:
- kind: ServiceAccount
  name: ohnod
  namespace: ohno
---
apiVersion: apps/v1
kind: DaemonSet # 负责请求 api server 与请求 ETCD 集群
metadata:
  name: ohnod
  namespace: ohno
  labels:
    app: ohnod
spec:
  selector:
    matchLabels:
      app: ohnod
  template:
    metadata:
      labels:
        app: ohnod
    spec:
      serviceAccountName: ohnod
      hostNetwork: true
      automountServiceAccountToken: true
      containers:
      - name: ohnod
        image: ohno/ohnod:latest
        command: ["/app/ohno/ohnod", "--loglevel", "debug"]
        imagePullPolicy: Never
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
        env:
        - name: ETCDCTL_CACERT
          value: "/etc/etcd-certs/ca.crt"
        - name: ETCDCTL_CERT
          value: "/etc/etcd-certs/client.crt"
        - name: ETCDCTL_KEY
          value: "/etc/etcd-certs/client.key"
        - name: ETCDCTL_API
          value: "3"
        securityContext:
          privileged: true
          capabilities:
            add:
            - NET_ADMIN # 需要添加静态路由
        volumeMounts:
        - name: api-server-access
          mountPath: /var/run/ohno
          readOnly: false  # Daemon Set 负责维护 token 并向保存宿主机供 CNI 插件使用
        - name: cni-config # 获取路由模式
          mountPath: /etc/cni/net.d/ohno.json
          readOnly: true
        - name: kubelet-conf # 获取 master 节点地址
          mountPath: /etc/kubernetes/kubelet.conf
          readOnly: true
        - name: etcd-certs-volume
          mountPath: /etc/etcd-certs
          readOnly: true
      volumes:
      - name: api-server-access
        hostPath:
          path: /var/run/ohno
          type: DirectoryOrCreate
      - name: cni-config
        hostPath:
          path: /etc/cni/net.d/ohno.json
          type: File
      - name: kubelet-conf
        hostPath:
          path: /etc/kubernetes/kubelet.conf
          type: File
      - name: etcd-certs-volume
        secret:
          secretName: etcd-certs
          defaultMode: 420
          items:
          - key: ca.crt
            path: ca.crt
          - key: client.crt
            path: client.crt
          - key: client.key
            path: client.key
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: rr
  namespace: ohno
  labels:
    app: frr-rr
spec:
  selector:
    matchLabels:
      app: frr-rr
  template:
    metadata:
      labels:
        app: frr-rr
    spec:
      hostNetwork: true
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/control-plane: ""
      containers:
      - name: frr-rr
        image: quay.io/frrouting/frr:8.4.5
        securityContext:
          privileged: true
        command: 
        - /bin/bash
        - -c
        - |
          set -ex
          
          echo "Configuring Spine Route Reflector with local segment: \"$SEGMENT\""
          sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons
          sed -i 's/bgpd_options="  -A 127.0.0.1"/bgpd_options="   -A 127.0.0.1 -f \/etc\/frr\/bgpd.conf"/' /etc/frr/daemons
          cat > /etc/frr/vtysh.conf << EOF
          no service integrated-vtysh-config
          EOF

          # 启动 FRR
          /usr/lib/frr/frrinit.sh start
          
          # 应用 Spine 配置
          vtysh -c "configure terminal" \
            -c "router bgp 64512" \
            -c " no bgp ebgp-requires-policy" \
            -c " no bgp network import-check" \
            -c " no bgp default ipv4-unicast" \
            -c " neighbor fabric peer-group" \
            -c " neighbor fabric remote-as internal" \
            -c " bgp listen range $SEGMENT peer-group fabric" \
            -c " address-family ipv4 unicast" \
            -c "  neighbor fabric activate" \
            -c "  neighbor fabric route-reflector-client" \
            -c " exit-address-family" \
            -c " address-family l2vpn evpn" \
            -c "  neighbor fabric activate" \
            -c "  neighbor fabric route-reflector-client" \
            -c "  advertise-all-vni" \
            -c "  advertise-svi-ip" \
            -c " exit-address-family" \
            -c "end" \
            -c "write"
          
          echo "Spine Route Reflector configuration completed"
          
          # 保持容器运行
          tail -f /dev/null
          set +x
      initContainers:
      - name: sysctl
        image: docker.io/library/busybox:latest
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'sysctl -w net.ipv4.ip_forward=1 && sysctl -w net.ipv6.conf.all.forwarding=1']
        securityContext:
          privileged: true
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: peer
  namespace: ohno
  labels:
    app: frr-peer
spec:
  selector:
    matchLabels:
      app: frr-peer
  template:
    metadata:
      labels:
        app: frr-peer
    spec:
      hostNetwork: true
      # Worker 节点没有污点，所以不需要 tolerations
      # 使用亲和性选择非 Master 节点
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: DoesNotExist
      containers:
      - name: frr-peer
        image: quay.io/frrouting/frr:8.4.5
        securityContext:
          privileged: true
        command: 
        - /bin/bash
        - -c
        - |
          set -ex

          sed -i 's/bgpd=no/bgpd=yes/' /etc/frr/daemons
          sed -i 's/bgpd_options="  -A 127.0.0.1"/bgpd_options="   -A 127.0.0.1 -f \/etc\/frr\/bgpd.conf"/' /etc/frr/daemons
          cat > /etc/frr/vtysh.conf << EOF
          no service integrated-vtysh-config
          EOF

          # 启动 FRR
          /usr/lib/frr/frrinit.sh start
          
          # 等待网络就绪
          sleep 5
          
          echo "Configuring Leaf BGP Peer with Spine IP: $SPINE_IP"
          
          # 应用 Leaf 配置
          vtysh -c "configure terminal" \
            -c "vrf ohnovrf" \
            -c " vni 42" \
            -c "exit-vrf" \
            -c "router bgp 64512" \
            -c " no bgp ebgp-requires-policy" \
            -c " no bgp network import-check" \
            -c " no bgp default ipv4-unicast" \
            -c " neighbor $SPINE_IP remote-as internal" \
            -c " neighbor $SPINE_IP allowas-in origin" \
            -c " address-family ipv4 unicast" \
            -c "  neighbor $SPINE_IP activate" \
            -c " exit-address-family" \
            -c " address-family l2vpn evpn" \
            -c "  neighbor $SPINE_IP activate" \
            -c "  neighbor $SPINE_IP allowas-in origin" \
            -c "  advertise-all-vni" \
            -c "  advertise-svi-ip" \
            -c " exit-address-family" \
            -c "router bgp 64512 vrf ohnovrf" \
            -c " address-family ipv4 unicast" \
            -c "  redistribute connected" \
            -c " exit-address-family" \
            -c " address-family ipv6 unicast" \
            -c "  redistribute static" \
            -c " exit-address-family" \
            -c " address-family l2vpn evpn" \
            -c "  advertise ipv4 unicast" \
            -c "  advertise ipv6 unicast" \
            -c " exit-address-family" \
            -c "end" \
            -c "write"
          
          echo "Leaf BGP Peer configuration completed"
          
          # 保持容器运行
          tail -f /dev/null
          set +x
      initContainers:
      - name: sysctl
        image: docker.io/library/busybox:latest
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'sysctl -w net.ipv4.ip_forward=1 && sysctl -w net.ipv6.conf.all.forwarding=1']
        securityContext:
          privileged: true